{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EsmaeilNarimissa/Dialectal-Retrieval-Bias/blob/main/aave_dataset_generation_deep_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8644fe41",
      "metadata": {
        "id": "8644fe41"
      },
      "source": [
        "# **Phase 1: AAVE/SAE Dataset Generation**\n",
        "\n",
        "This notebook implements a complete pipeline for generating a hybrid dataset containing both Standard American English (SAE) and African American Vernacular English (AAVE) queries for research on dialectal bias in RAG systems.\n",
        "\n",
        "**Overview**\n",
        "\n",
        "The notebook is structured in three main phases:\n",
        "1. **Setup and Configuration**: Import libraries, load API keys, and define constants\n",
        "2. **Data Sourcing**: Load and filter the SQuAD dataset for suitable queries\n",
        "3. **Synthetic Generation**: Use LLM API calls to convert SAE queries to AAVE variants\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "- OpenAI API key set as environment variable `OPENAI_API_KEY`\n",
        "- Sufficient API credits for the generation process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18a7ddb",
      "metadata": {
        "id": "d18a7ddb"
      },
      "source": [
        "## Step 1: Setup and Configuration\n",
        "\n",
        "Setting up the environment with necessary libraries, API keys, and configuration constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e310003",
      "metadata": {
        "id": "6e310003"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q datasets openai tqdm pandas numpy openai\n",
        "\n",
        "print(\"Packages installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9e8c14",
      "metadata": {
        "id": "ac9e8c14"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "import sys # Added for Python version check\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "\n",
        "# Load API keys and configuration\n",
        "# Use Google Colab's user data secrets to securely store the API key\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"ERROR: OPENAI_API_KEY not found in Colab secrets!\")\n",
        "    print(\"Please set your OpenAI API key in the Colab Secrets tab (icon on the left).\")\n",
        "else:\n",
        "    print(\"OpenAI API key loaded successfully from Colab secrets\")\n",
        "\n",
        "# Initialize OpenAI client with the modern SDK\n",
        "from openai import OpenAI, __version__ as openai_version # Import necessary classes/versions\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "print(\"OpenAI Client initialized.\")\n",
        "\n",
        "# Verify OpenAI SDK and Python versions\n",
        "print(\"\\nPython:\", sys.version) # Use sys for Python version\n",
        "try:\n",
        "    print(\"OpenAI SDK version:\", openai_version) # Use imported openai_version\n",
        "except Exception as e:\n",
        "    print(\"Could not read OpenAI SDK version:\", repr(e))\n",
        "\n",
        "# Legacy version attribute fallback - for older installs if needed\n",
        "# print(\"openai version (legacy attr):\", getattr(openai, \"__version__\", \"N/A\")) # This might not be needed with modern SDK import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = \"gpt-4.1-mini\"  # change if your org uses different names\n",
        "print(\"Testing model:\", test_model)\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=test_model,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say 'hello'?\"}],\n",
        "    max_completion_tokens=10,\n",
        ")\n",
        "print(\"Response:\", repr(resp.choices[0].message.content))"
      ],
      "metadata": {
        "id": "jW4iXKtP_ot6"
      },
      "id": "jW4iXKtP_ot6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Completions Model Probe\n",
        "Test a shortlist of chat.completions models, log success/failure and a sample reply.\n",
        "\n",
        "**OpenAI Pricing per 1M tokens (27/Sep/25):**\n",
        "\n",
        "| Model             | Input (USD) | Cached input (USD) | Output (USD) |\n",
        "|-------------------|-----------|------------------|------------|\n",
        "| gpt-5             | 1.25      | 0.125            | 10.00      |\n",
        "| gpt-5-mini        | 0.25      | 0.025            | 2.00       |\n",
        "| gpt-5-nano        | 0.05      | 0.005            | 0.40       |\n",
        "| gpt-5-chat-latest | 1.25      | 0.125            | 10.00      |\n",
        "| gpt-5-codex       | 1.25      | 0.125            | 10.00      |\n",
        "| gpt-4.1           | 2.00      | 0.50             | 8.00       |\n",
        "| gpt-4.1-mini      | 0.40      | 0.10             | 1.60       |\n",
        "| gpt-4.1-nano      | 0.10      | 0.025            | 0.40       |\n",
        "| gpt-4o            | 2.50      | 1.25             | 10.00      |\n"
      ],
      "metadata": {
        "id": "W5OqpEXfkoe_"
      },
      "id": "W5OqpEXfkoe_"
    },
    {
      "cell_type": "code",
      "source": [
        "CANDIDATE_MODELS = [\n",
        "    \"gpt-5\",         # top-tier quality\n",
        "    \"gpt-4o\",        # strong multimodal family, robust chat-completions\n",
        "    \"gpt-4.1\",       # strong reasoning, broadly supported\n",
        "    \"gpt-4o-mini\",   # economical, good quality\n",
        "    \"gpt-4.1-mini\",  # economical, known to work (we verified)\n",
        "    \"gpt-5-mini\",    # cheap, may have endpoint constraints in some orgs\n",
        "]\n",
        "\n",
        "def probe_chat_models(models: list[str], max_completion_tokens: int = 8) -> dict:\n",
        "    \"\"\"\n",
        "    Try a minimal chat completion on each model and report status and sample text.\n",
        "    Returns a dict: {model: {\"ok\": bool, \"error\": str|None, \"sample\": str|None}}\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for m in models:\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=m,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Reply with one short word only.\"},\n",
        "                    {\"role\": \"user\", \"content\": \"Hello?\"},\n",
        "                ],\n",
        "                max_completion_tokens=max_completion_tokens,\n",
        "            )\n",
        "            content = (resp.choices[0].message.content or \"\").strip()\n",
        "            ok = bool(content)\n",
        "            results[m] = {\"ok\": ok, \"error\": None if ok else \"Empty content\", \"sample\": content if ok else None}\n",
        "            print(f\"[OK] {m}: {repr(content)}\" if ok else f\"[EMPTY] {m}\")\n",
        "        except Exception as e:\n",
        "            # Capture concise error\n",
        "            results[m] = {\"ok\": False, \"error\": str(e), \"sample\": None}\n",
        "            print(f\"[ERR] {m}: {e}\")\n",
        "    return results\n",
        "\n",
        "print(\"Probing chat-completions models...\")\n",
        "model_probe_results = probe_chat_models(CANDIDATE_MODELS)\n",
        "print(\"\\nSummary:\")\n",
        "for m, r in model_probe_results.items():\n",
        "    status = \"OK\" if r[\"ok\"] else \"FAIL\"\n",
        "    sample = f\" sample={repr(r['sample'])}\" if r[\"sample\"] else \"\"\n",
        "    err = f\" error={r['error']}\" if r[\"error\"] else \"\"\n",
        "    print(f\"- {m}: {status}{sample}{err}\")"
      ],
      "metadata": {
        "id": "Bgg-GvGyBc1I"
      },
      "id": "Bgg-GvGyBc1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation:\n",
        "\n",
        "- Primary: gpt-4.1-mini — best price/quality balance; it works.\n",
        "- Alternative higher quality: gpt-4.1 or gpt-4o — more expensive, potentially slightly better linguistic nuance.\n",
        "- Budget option: gpt-4o-mini — cheap and capable; if results look fine on a small sample, you can scale with it."
      ],
      "metadata": {
        "id": "abTGWJqOCFMY"
      },
      "id": "abTGWJqOCFMY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c63bce8",
      "metadata": {
        "id": "0c63bce8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo  # Python 3.9+\n",
        "\n",
        "# Change to your local tz name if different\n",
        "LOCAL_TZ = ZoneInfo(\"Australia/Sydney\")\n",
        "RUN_ID = datetime.now(LOCAL_TZ).strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Configuration constants\n",
        "CONFIG = {\n",
        "    # Dataset configuration\n",
        "    'dataset_name': 'squad',\n",
        "    'dataset_split': 'train',\n",
        "    'sample_size': 200,  # Number of queries to process\n",
        "    'min_query_length': 10,\n",
        "    'max_query_length': 200,  # A typical question is between 30-80 characters!\n",
        "\n",
        "    # Output configuration\n",
        "    'output_file': f\"aave_poc_dataset_{RUN_ID}.json\", # Using the dynamic RUN_ID\n",
        "    # Note: BASE_OUTPUT was removed as it's not defined or used consistently\n",
        "\n",
        "    # Model configuration\n",
        "    'model_name': 'gpt-4.1-mini', # or \"gpt-4.1\" / \"gpt-4o\" / \"gpt-4o-mini\"\n",
        "\n",
        "    # API call configuration\n",
        "    'max_retries': 3,\n",
        "    'retry_delay': 1,  # seconds\n",
        "\n",
        "    # Progress tracking configuration\n",
        "    'batch_size': 10,  # Progress tracking in the original notebook\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52cf134",
      "metadata": {
        "id": "a52cf134"
      },
      "source": [
        "## Step 2: Data Sourcing\n",
        "\n",
        "Loading the SQuAD dataset and filtering suitable queries for AAVE conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2e5988",
      "metadata": {
        "id": "1b2e5988"
      },
      "outputs": [],
      "source": [
        "# Load SQuAD dataset\n",
        "print(\"Loading SQuAD dataset...\")\n",
        "try:\n",
        "    dataset = load_dataset(CONFIG['dataset_name'], split=CONFIG['dataset_split'])\n",
        "    print(f\"Dataset loaded successfully: {len(dataset)} examples\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baaa36a1",
      "metadata": {
        "id": "baaa36a1"
      },
      "outputs": [],
      "source": [
        "# Extract and filter queries\n",
        "def extract_queries(dataset) -> List[Tuple[int, str]]:\n",
        "    \"\"\"Extract representative, high-quality SQuAD questions and their indices for AAVE conversion.\"\"\"\n",
        "    queries = []\n",
        "    seen = set()\n",
        "\n",
        "    for i, ex in enumerate(dataset):\n",
        "        q = ex[\"question\"].strip()\n",
        "        q_lower = q.lower()\n",
        "\n",
        "        # Quality checks\n",
        "        if not (CONFIG[\"min_query_length\"] <= len(q) <= CONFIG[\"max_query_length\"]):\n",
        "            continue\n",
        "        if not q.endswith(\"?\"):\n",
        "            continue\n",
        "        if len(q.split()) < 3:\n",
        "            continue\n",
        "\n",
        "        # Deduplicate (case-insensitive)\n",
        "        if q_lower in seen:\n",
        "            continue\n",
        "        seen.add(q_lower)\n",
        "\n",
        "        queries.append((i, q)) # Store as (index, question) tuple\n",
        "\n",
        "    return queries\n",
        "\n",
        "print(\"Extracting and filtering queries...\")\n",
        "all_queries = extract_queries(dataset)\n",
        "print(f\"Extracted {len(all_queries)} suitable (idx, question) pairs\")\n",
        "\n",
        "# Sample queries for processing\n",
        "if len(all_queries) > CONFIG['sample_size']:\n",
        "    selected_queries = random.sample(all_queries, CONFIG['sample_size'])\n",
        "    print(f\"Randomly sampled {CONFIG['sample_size']} pairs for processing\")\n",
        "else:\n",
        "    selected_queries = all_queries\n",
        "    print(f\"Using all {len(selected_queries)} available pairs\")\n",
        "\n",
        "# Display sample queries\n",
        "print(\"\\n Sample queries:\")\n",
        "# Displaying tuples (index, question)\n",
        "for i, (idx, query) in enumerate(selected_queries[:5]):\n",
        "    print(f\"  {i+1}. (Index: {idx}) {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a512a95b",
      "metadata": {
        "id": "a512a95b"
      },
      "source": [
        "## Step 3: Synthetic Generation\n",
        "\n",
        "Converting SAE queries to AAVE variants using LLM API calls with proper error handling and progress tracking."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Prompt Engineering"
      ],
      "metadata": {
        "id": "-N-B9TxMS4_t"
      },
      "id": "-N-B9TxMS4_t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b517cf1f",
      "metadata": {
        "id": "b517cf1f"
      },
      "outputs": [],
      "source": [
        "# AAVE conversion prompt template\n",
        "AAVE_CONVERSION_PROMPT = \"\"\"You are a linguist specializing in African American Vernacular English (AAVE).\n",
        "\n",
        "Convert the following Standard American English (SAE) question into a natural AAVE equivalent suitable for everyday speech, preserving the exact meaning.\n",
        "\n",
        "Strict requirements:\n",
        "- Preserve named entities, numbers, dates, and factual content exactly.\n",
        "- Preserve tense/aspect and auxiliaries unless clearly required for natural AAVE.\n",
        "- If the question encodes time (e.g., years/dates), maintain the same temporal cues.\n",
        "- Keep it a single question ending with one question mark.\n",
        "- Maintain clarity; prefer minimal edits over heavy rewrites.\n",
        "- Do not add or remove information; do not change intent.\n",
        "- Avoid phonetic spellings (e.g., droppin’ → dropping) and avoid stereotypes or caricature.\n",
        "\n",
        "Guidance on AAVE features (use only when natural and subtle):\n",
        "- Copula deletion when natural (e.g., “He tall”) but do not drop “been” when perfect aspect is intended.\n",
        "- Habitual “be” for habitual actions (e.g., “She be working”) only when the SAE implies habit.\n",
        "- Multiple negation when natural (e.g., “I don’t know nothing”) without changing meaning.\n",
        "- AAVE-consistent verb patterns (e.g., “You was”) only when they do not alter tense/aspect.\n",
        "- Light lexical shifts (e.g., “cause” → “’cause”) are acceptable but avoid changing register excessively.\n",
        "- Retain WH-question support auxiliaries (did/does/do) when required for clarity; do not produce “did … got” combinations.\n",
        "\n",
        "Output only the AAVE question (single line, no quotes, no explanations).\n",
        "\n",
        "SAE Question: {sae_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"AAVE conversion prompt template defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 AAVE Conversion via OpenAI Chat Completions (Modern SDK)\n",
        "\n",
        "Defines a robust helper that converts SAE questions to natural AAVE using the openai>=1.0 chat.completions API with retries and basic validation."
      ],
      "metadata": {
        "id": "2LWBbGdC36BU"
      },
      "id": "2LWBbGdC36BU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96850412",
      "metadata": {
        "id": "96850412"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def convert_to_aave(sae_query: str, max_retries: int = CONFIG['max_retries']) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Convert an SAE question to a natural AAVE variant using OpenAI Chat Completions (modern SDK).\n",
        "\n",
        "    Args:\n",
        "        sae_query: The original SAE question text.\n",
        "        max_retries: Number of retry attempts on transient API errors.\n",
        "\n",
        "    Returns:\n",
        "        The AAVE-transformed question (string) if valid; otherwise None.\n",
        "    \"\"\"\n",
        "    prompt = AAVE_CONVERSION_PROMPT.format(sae_query=sae_query)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Modern SDK call: client.chat.completions.create(...)\n",
        "            resp = client.chat.completions.create(\n",
        "                model=CONFIG['model_name'],\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_completion_tokens=150,\n",
        "                # temperature=0.7,\n",
        "                # top_p=0.9,\n",
        "            )\n",
        "\n",
        "            # Extract the assistant message\n",
        "            aave_query = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "            # Basic validation to ensure we got a proper question back\n",
        "            if aave_query and len(aave_query) > 5 and aave_query.endswith(\"?\"):\n",
        "                return aave_query\n",
        "            else:\n",
        "                print(f\"Invalid response for '{sae_query[:50]}...': {aave_query}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed for '{sae_query[:50]}...': {e}\")\n",
        "            # Exponential backoff: delay grows linearly with attempt count multiplier\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(CONFIG['retry_delay'] * (attempt + 1))\n",
        "            else:\n",
        "                print(f\"Failed to convert after {max_retries} attempts: {sae_query}\")\n",
        "                return None\n",
        "\n",
        "    return None\n",
        "\n",
        "print(\"AAVE conversion function defined with retry logic\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 5 SAE queries from your selected set (or all_queries if you prefer)\n",
        "spot_sample = random.sample(selected_queries, k=min(5, len(selected_queries)))\n",
        "print(\"\\nSpot check with updated prompt:\")\n",
        "for i, q in enumerate(spot_sample, 1):\n",
        "    aave = convert_to_aave(q)\n",
        "    print(f\"\\n[{i}]\")\n",
        "    print(\"SAE:\", q)\n",
        "    print(\"AAVE:\", aave)"
      ],
      "metadata": {
        "id": "Jt5VBLX5H4Fc"
      },
      "id": "Jt5VBLX5H4Fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Hybrid Dataset Generation Loop (SAE→AAVE) with Progress Logging\n",
        "Iterates over SAE queries, generates AAVE variants with retries, accumulates results, and reports periodic progress. Returns data and success/failure counts."
      ],
      "metadata": {
        "id": "eqMxx_9X4obw"
      },
      "id": "eqMxx_9X4obw"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hybrid_dataset(idx_q_pairs: List[Tuple[int, str]]) -> Tuple[List[Dict], int, int]:\n",
        "    \"\"\"\n",
        "    Generate a hybrid dataset of SAE and AAVE question pairs, retaining the original SQuAD index.\n",
        "\n",
        "    Args:\n",
        "        idx_q_pairs: List of (SQuAD index, SAE question) tuples to convert.\n",
        "\n",
        "    Returns:\n",
        "        hybrid_dataset: List of dict entries: id, squad_idx, sae_query, aave_query, source, conversion_method.\n",
        "        successful_conversions: Number of successful AAVE generations.\n",
        "        failed_conversions: Number of failed generations.\n",
        "    \"\"\"\n",
        "    hybrid_dataset: List[Dict] = []\n",
        "    successful_conversions = 0\n",
        "    failed_conversions = 0\n",
        "\n",
        "    print(f\"Starting generation of {len(idx_q_pairs)} query pairs...\")\n",
        "\n",
        "    # Process in batches for progress tracking\n",
        "    for i in tqdm(range(0, len(idx_q_pairs), CONFIG['batch_size']), desc=\"Processing batches\"):\n",
        "        batch = idx_q_pairs[i:i + CONFIG['batch_size']]\n",
        "\n",
        "        for squad_idx, sae_query in batch: # Unpack the tuple\n",
        "            aave_query = convert_to_aave(sae_query)\n",
        "            if aave_query:\n",
        "                entry = {\n",
        "                    \"id\": len(hybrid_dataset),\n",
        "                    \"squad_idx\": squad_idx,  # Include the original SQuAD index\n",
        "                    \"sae_query\": sae_query,\n",
        "                    \"aave_query\": aave_query,\n",
        "                    \"source\": \"squad\",\n",
        "                    \"conversion_method\": \"llm_synthetic\",\n",
        "                }\n",
        "                hybrid_dataset.append(entry)\n",
        "                successful_conversions += 1\n",
        "            else:\n",
        "                failed_conversions += 1\n",
        "\n",
        "            # Small delay to reduce rate-limiting risk; adjust if needed\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Periodic progress updates every 5 batches\n",
        "        batches_done = (i // CONFIG[\"batch_size\"]) + 1\n",
        "        if batches_done % 5 == 0:\n",
        "            total = successful_conversions + failed_conversions\n",
        "            rate = (successful_conversions / total * 100.0) if total else 0.0\n",
        "            print(\"Progress update:\")\n",
        "            print(f\"  Successful conversions: {successful_conversions}\")\n",
        "            print(f\"  Failed conversions: {failed_conversions}\")\n",
        "            print(f\"  Success rate: {rate:.1f}%\")\n",
        "\n",
        "\n",
        "    return hybrid_dataset, successful_conversions, failed_conversions\n",
        "\n",
        "\n",
        "# Run the generation process\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING HYBRID DATASET GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Pass the list of (index, question) tuples to the generation function\n",
        "hybrid_data, success_count, fail_count = generate_hybrid_dataset(selected_queries)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "total = success_count + fail_count\n",
        "overall_rate = (success_count / total * 100.0) if total else 0.0\n",
        "print(f\"Successfully converted: {success_count} queries\")\n",
        "print(f\"Failed conversions: {fail_count} queries\")\n",
        "print(f\"Overall success rate: {overall_rate:.1f}%\")\n",
        "print(f\"Total dataset size: {len(hybrid_data)} query pairs\")"
      ],
      "metadata": {
        "id": "dvLD9-4O4o6q"
      },
      "id": "dvLD9-4O4o6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 QA: Flag likely AAVE issues\n",
        "\n",
        "Scans the in-memory hybrid_data and flags entries with common issues (missing WH auxiliaries, “did … got” combo, and phonetic “in’” spellings). Prints a summary and the first 10 flagged examples."
      ],
      "metadata": {
        "id": "5ctqOquDPXR7"
      },
      "id": "5ctqOquDPXR7"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def flag_more(entries):\n",
        "    flags = []\n",
        "    for i, e in enumerate(entries):\n",
        "        sae = e[\"sae_query\"]\n",
        "        aave = e[\"aave_query\"]\n",
        "        sae_l = sae.lower()\n",
        "        aave_l = aave.lower()\n",
        "        issues = []\n",
        "\n",
        "        # 1) 'did ... got' ungrammatical combo\n",
        "        if re.search(r\"\\bdid\\b[^?]*\\bgot\\b\", aave_l):\n",
        "            issues.append(\"did_got_combo\")\n",
        "\n",
        "        # 2) WH + did/does/do in SAE but missing supportive aux in AAVE\n",
        "        if re.match(r\"^(what|when|where|why|how|which)\\b.*\\b(did|does|do)\\b\", sae_l):\n",
        "            if not re.search(r\"\\b(did|does|do|done)\\b\", aave_l):\n",
        "                issues.append(\"missing_aux_after_wh\")\n",
        "\n",
        "        # 3) Phonetic 'in’ or similar\n",
        "        if \"’\" in aave or \"'\" in aave:\n",
        "            # normalize straight apostrophes for check\n",
        "            a_norm = aave.replace(\"’\", \"'\")\n",
        "            if re.search(r\"\\b\\w+in'\\b\", a_norm):\n",
        "                issues.append(\"phonetic_spelling\")\n",
        "\n",
        "        if issues:\n",
        "            flags.append({\"idx\": i, \"issues\": issues, \"sae\": sae, \"aave\": e[\"aave_query\"]})\n",
        "    return flags\n",
        "\n",
        "more_flags = flag_more(hybrid_data)\n",
        "print(f\"Additional flags: {len(more_flags)}\")\n",
        "for f in more_flags[:10]:\n",
        "    print(\"\\nIndex:\", f[\"idx\"], \"Issues:\", \", \".join(f[\"issues\"]))\n",
        "    print(\"SAE :\", f[\"sae\"])\n",
        "    print(\"AAVE:\", f[\"aave\"])"
      ],
      "metadata": {
        "id": "JUQ0cgBQImDu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JUQ0cgBQImDu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.5 Auto-fix: Restore missing WH auxiliaries\n",
        "Automatically restores did/does/do after WH words when the SAE uses them but the AAVE dropped them. Applies fixes in-place to hybrid_data and reports how many were corrected."
      ],
      "metadata": {
        "id": "-UtzNv4sPsgP"
      },
      "id": "-UtzNv4sPsgP"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def restore_wh_aux(sae: str, aave: str) -> str:\n",
        "    \"\"\"\n",
        "    If SAE has a WH + (did|does|do), but AAVE lacks any support aux, restore it after the WH token.\n",
        "    Keeps the rest of the AAVE string unchanged.\n",
        "    \"\"\"\n",
        "    sae_l = sae.lower()\n",
        "    aave_l = aave.lower()\n",
        "\n",
        "    m = re.match(r\"^(what|when|where|why|how|which)\\b(.*?\\b)(did|does|do)\\b(.*)$\", sae_l)\n",
        "    if not m:\n",
        "        return aave  # Not our target pattern\n",
        "\n",
        "    wh = m.group(1)  # what/when/...\n",
        "    aux = m.group(3)  # did/does/do\n",
        "\n",
        "    # If AAVE already contains a support aux, leave it\n",
        "    if re.search(r\"\\b(did|does|do|done)\\b\", aave_l):\n",
        "        return aave\n",
        "\n",
        "    # Try to insert the aux after the first WH token in the AAVE string\n",
        "    # Pattern: start -> WH (...) -> rest\n",
        "    m2 = re.match(r\"^(?P<wh>\"+wh+r\")\\b(?P<rest>.*)$\", aave_l)\n",
        "    if not m2:\n",
        "        return aave\n",
        "\n",
        "    # Reconstruct with inserted aux (preserve original casing/punctuation from AAVE)\n",
        "    # Use original AAVE to avoid lowercasing the entire string\n",
        "    # Find WH in the original AAVE (case-insensitive)\n",
        "    def ci_find(haystack, needle):\n",
        "        hl = haystack.lower()\n",
        "        nl = needle.lower()\n",
        "        i = hl.find(nl)\n",
        "        return i\n",
        "\n",
        "    i = ci_find(aave, wh)\n",
        "    if i == -1:\n",
        "        return aave\n",
        "\n",
        "    j = i + len(wh)\n",
        "    # Insert single space + aux after WH\n",
        "    fixed = aave[:j] + f\" {aux}\" + aave[j:]\n",
        "    # Clean extra spaces like “What  the …”\n",
        "    fixed = re.sub(r\"\\s{2,}\", \" \", fixed)\n",
        "    return fixed\n",
        "\n",
        "def autofix_missing_aux(entries, flags):\n",
        "    count = 0\n",
        "    for f in flags:\n",
        "        if \"missing_aux_after_wh\" in f[\"issues\"]:\n",
        "            idx = f[\"idx\"]\n",
        "            sae = entries[idx][\"sae_query\"]\n",
        "            aave = entries[idx][\"aave_query\"]\n",
        "            fixed = restore_wh_aux(sae, aave)\n",
        "            if fixed != aave:\n",
        "                entries[idx][\"aave_query\"] = fixed\n",
        "                count += 1\n",
        "    print(f\"Applied {count} WH-aux restorations.\")\n",
        "\n",
        "# Apply\n",
        "autofix_missing_aux(hybrid_data, more_flags)"
      ],
      "metadata": {
        "id": "raOx47orNuIz"
      },
      "id": "raOx47orNuIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.6 Re-check: Post-fix QA summary\n",
        "Re-runs the QA on hybrid_data after the auto-fix to confirm issues are resolved. Prints the new count and samples if any remain."
      ],
      "metadata": {
        "id": "REOPWafTP2ud"
      },
      "id": "REOPWafTP2ud"
    },
    {
      "cell_type": "code",
      "source": [
        "more_flags = flag_more(hybrid_data)\n",
        "print(f\"Additional flags after WH-aux fix: {len(more_flags)}\")\n",
        "for f in more_flags[:10]:\n",
        "    print(\"\\nIndex:\", f[\"idx\"], \"Issues:\", \", \".join(f[\"issues\"]))\n",
        "    print(\"SAE :\", f[\"sae\"])\n",
        "    print(\"AAVE:\", f[\"aave\"])"
      ],
      "metadata": {
        "id": "bVvn1gAMN6Bk"
      },
      "id": "bVvn1gAMN6Bk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Save dataset and write run statistics\n",
        "Writes the in-memory hybrid_data to a JSON file and saves a companion _stats.json with summary metrics (pair count, success/fail counts, success rate, source dataset, model used, and timestamp). Assumes success_count and fail_count exist in the current scope."
      ],
      "metadata": {
        "id": "lwlCkZa8QIIC"
      },
      "id": "lwlCkZa8QIIC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d5b8f6",
      "metadata": {
        "id": "f5d5b8f6"
      },
      "outputs": [],
      "source": [
        "# Save the hybrid dataset\n",
        "def save_dataset(data: List[Dict], filename: str):\n",
        "    \"\"\"Save the hybrid dataset to JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Dataset saved successfully to {filename}\")\n",
        "\n",
        "        # Save summary statistics\n",
        "        stats = {\n",
        "            'total_pairs': len(data),\n",
        "            'successful_conversions': success_count,\n",
        "            'failed_conversions': fail_count,\n",
        "            'success_rate': success_count/(success_count+fail_count)*100,\n",
        "            'source_dataset': CONFIG['dataset_name'],\n",
        "            'model_used': CONFIG['model_name'],\n",
        "            'generation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        stats_filename = filename.replace('.json', '_stats.json')\n",
        "        with open(stats_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"Statistics saved to {stats_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving dataset: {e}\")\n",
        "\n",
        "# Save the dataset\n",
        "save_dataset(hybrid_data, CONFIG['output_file'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 Sample outputs and quick dataset stats\n",
        "Prints the first 5 SAE→AAVE pairs for a visual spot check, then reports simple length-based metrics (average lengths and their difference) across the full in-memory dataset."
      ],
      "metadata": {
        "id": "iedYMHpgQfO3"
      },
      "id": "iedYMHpgQfO3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0390a98",
      "metadata": {
        "id": "b0390a98"
      },
      "outputs": [],
      "source": [
        "# Display sample results\n",
        "print(\"\\nSAMPLE RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if hybrid_data:\n",
        "    # Show first 5 examples\n",
        "    for i, entry in enumerate(hybrid_data[:5]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"  SAE:  {entry['sae_query']}\")\n",
        "        print(f\"  AAVE: {entry['aave_query']}\")\n",
        "        print(f\"  ID:   {entry['id']}\")\n",
        "\n",
        "    print(f\"\\n... and {len(hybrid_data) - 5} more pairs\")\n",
        "\n",
        "    # Basic analysis\n",
        "    sae_lengths = [len(entry['sae_query']) for entry in hybrid_data]\n",
        "    aave_lengths = [len(entry['aave_query']) for entry in hybrid_data]\n",
        "\n",
        "    print(\"\\nDATASET ANALYSIS\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Average SAE query length:  {np.mean(sae_lengths):.1f} characters\")\n",
        "    print(f\"Average AAVE query length: {np.mean(aave_lengths):.1f} characters\")\n",
        "    print(f\"Length difference:         {np.mean(aave_lengths) - np.mean(sae_lengths):+.1f} characters\")\n",
        "else:\n",
        "    print(\"No data generated. Please check the errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66576e9c",
      "metadata": {
        "id": "66576e9c"
      },
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "\n",
        "The hybrid SAE↔AAVE dataset is complete, cleaned, and ready for evaluation. We achieved 200/200 conversions, applied a targeted WH-auxiliary auto-fix (37 items), and confirmed zero residual QA flags. Length delta is minimal, helping control for confounds.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "1) Final QA and minor touch-ups\n",
        "- Manually correct any remaining edge cases (e.g., unnecessary habitual “be” when not implied).\n",
        "- Re-run the sample/analysis cell to confirm consistency.\n",
        "\n",
        "2) Save and version\n",
        "- Save with a timestamped filename (and timezone) plus stats for reproducibility.\n",
        "\n",
        "3) RAG evaluation\n",
        "- Retriever: Compare recall/precision@k for SAE vs AAVE queries on the same corpus.\n",
        "- Generator: Compare exact match/F1 (and calibration) across dialects.\n",
        "- Significance: Use paired tests (e.g., McNemar for accuracy; Wilcoxon for continuous metrics).\n",
        "\n",
        "4) Iterate\n",
        "- If you see systematic errors (e.g., tense/aspect drift), refine the prompt slightly and regenerate only affected items.\n",
        "- Keep the WH-aux restore as a standard post-processing step.\n",
        "\n",
        "**Files Generated:**\n",
        "\n",
        "- aave_poc_dataset_YYYYMMDD-HHMMSS.json (final dataset)\n",
        "- aave_poc_dataset_YYYYMMDD-HHMMSS_stats.json (run metadata and counts)\n",
        "\n",
        "**How to use this dataset in RAG bias tests**\n",
        "\n",
        "- Fair pairing: For each SAE query, use its AAVE counterpart against the same index.\n",
        "- Retriever bias: Measure hit rate/recall@k and rank positions for SAE vs AAVE.\n",
        "- Generator bias: Measure EM/F1 and factuality given the same retrieved context.\n",
        "- Report deltas with confidence intervals and p-values; document prompt version and QA auto-fixes used.\n",
        "\n",
        "This closes the dataset phase with strong quality controls and clear auditability. Proceed to the RAG experiments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to your notebook on Google Drive\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/aave_dataset_generation-2.ipynb'\n",
        "\n",
        "# Define the path for the cleaned output file\n",
        "cleaned_path = '/content/drive/MyDrive/Colab Notebooks/aave_dataset_generation-2-cleaned.ipynb'\n",
        "\n",
        "try:\n",
        "    # Load the notebook file\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        nb = json.load(f)\n",
        "\n",
        "    # A flag to track if any changes were made\n",
        "    metadata_cleaned = False\n",
        "\n",
        "    # Iterate through each cell in the notebook\n",
        "    for cell in nb.get('cells', []):\n",
        "        # Check for and remove the 'widgets' metadata key\n",
        "        if 'widgets' in cell.get('metadata', {}):\n",
        "            del cell['metadata']['widgets']\n",
        "            metadata_cleaned = True\n",
        "\n",
        "        # Clean up execution-related metadata that can also be problematic\n",
        "        if 'execution' in cell.get('metadata', {}):\n",
        "            cell['metadata']['execution'] = {}\n",
        "            metadata_cleaned = True\n",
        "\n",
        "    # Save the cleaned notebook to the new path\n",
        "    with open(cleaned_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(nb, f, indent=1)\n",
        "\n",
        "    if metadata_cleaned:\n",
        "        print(f\"Successfully cleaned the notebook. The new file is saved at:\\n{cleaned_path}\")\n",
        "    else:\n",
        "        print(f\"No problematic metadata was found. A clean copy has been saved at:\\n{cleaned_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at the specified path:\\n{path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "02ExloM1DucT"
      },
      "id": "02ExloM1DucT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2XXAsZfDERRq"
      },
      "id": "2XXAsZfDERRq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
